{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 100), dtype=float32, numpy=\n",
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = tf.keras.layers.Dense(100)\n",
    "layer = tf.keras.layers.Dense(100,input_shape = (None,5))\n",
    "layer(tf.zeros([3,5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'dense_7/kernel:0' shape=(5, 100) dtype=float32, numpy=\n",
       " array([[ 1.10843793e-01, -1.22447059e-01,  2.52075344e-02,\n",
       "          9.04147476e-02, -6.48622960e-02, -1.75273016e-01,\n",
       "         -1.37443960e-01,  2.02868894e-01,  2.89248973e-02,\n",
       "          3.64692062e-02, -1.52013123e-01,  2.80783623e-02,\n",
       "          1.78069845e-01, -3.23069692e-02,  8.88404399e-02,\n",
       "          2.59214789e-02,  3.39988619e-02, -1.30343437e-02,\n",
       "          2.06639931e-01,  6.25833720e-02,  8.13232362e-03,\n",
       "          2.09118798e-01, -1.62215769e-01,  1.30225345e-01,\n",
       "          1.11125365e-01, -1.21910721e-02, -4.06540930e-02,\n",
       "          2.03676373e-02,  2.20266595e-01, -1.80565566e-02,\n",
       "         -2.16052830e-02, -1.69636518e-01,  1.45049497e-01,\n",
       "         -2.34577194e-01,  1.09925315e-01, -1.90073058e-01,\n",
       "         -4.37143892e-02, -2.23394841e-01, -6.42518997e-02,\n",
       "          6.69075996e-02, -8.79259706e-02,  2.08262458e-01,\n",
       "         -2.31247887e-01,  2.57948786e-02, -9.18564349e-02,\n",
       "         -9.18298811e-02,  2.00986192e-01,  4.91460413e-02,\n",
       "          8.26503187e-02,  1.29991338e-01, -8.56688321e-02,\n",
       "         -2.27528423e-01,  1.07959792e-01,  2.85352319e-02,\n",
       "         -4.15598750e-02,  1.06466964e-01, -2.27778792e-01,\n",
       "         -8.82050097e-02,  2.19237193e-01,  1.47422954e-01,\n",
       "          8.95149857e-02,  2.27172032e-01, -1.42482698e-01,\n",
       "          1.77204505e-01, -3.95187289e-02,  1.68357030e-01,\n",
       "         -4.10866737e-02,  2.44677514e-02, -1.64605081e-01,\n",
       "         -8.65355134e-02,  1.88850626e-01, -2.31448799e-01,\n",
       "         -1.23012714e-01,  2.28242084e-01,  1.30742341e-02,\n",
       "         -6.63207471e-02, -1.28714114e-01,  4.85761613e-02,\n",
       "         -7.51079172e-02,  7.58397728e-02,  6.20642304e-03,\n",
       "          1.72286019e-01, -1.04165375e-01,  1.24849245e-01,\n",
       "         -7.45871216e-02,  1.34786025e-01,  2.28142455e-01,\n",
       "          1.56275377e-01, -2.12182611e-01, -2.22305954e-03,\n",
       "          2.12509140e-01,  1.37818739e-01,  1.42191932e-01,\n",
       "         -7.17578828e-02, -1.23335347e-01,  2.47346610e-02,\n",
       "         -3.04381549e-03, -1.84358597e-01, -6.50027394e-03,\n",
       "          2.02306882e-01],\n",
       "        [ 1.50109217e-01,  2.60302871e-02,  2.30452314e-01,\n",
       "          3.25347632e-02,  5.33261448e-02,  1.54889092e-01,\n",
       "         -4.42259461e-02,  8.17080885e-02,  1.57935992e-01,\n",
       "         -1.12476647e-01, -1.12285107e-01,  1.42938808e-01,\n",
       "          5.09825349e-03,  8.25136453e-02, -3.72909307e-02,\n",
       "         -4.77517694e-02, -8.50712508e-02,  1.81189343e-01,\n",
       "         -2.73434520e-02,  2.20017284e-02,  5.81039637e-02,\n",
       "          6.86147660e-02, -2.01499343e-01, -1.33144051e-01,\n",
       "          1.17997959e-01, -1.88388690e-01,  2.29842439e-01,\n",
       "         -2.38078609e-01,  1.12121806e-01, -8.03190470e-03,\n",
       "          1.57892063e-01,  1.28679052e-01,  1.13071367e-01,\n",
       "          1.73953637e-01,  1.05934665e-01,  2.28262886e-01,\n",
       "          2.00669363e-01, -2.20045358e-01, -1.65049851e-01,\n",
       "          2.06436828e-01,  1.22708872e-01, -1.50521666e-01,\n",
       "          1.06381670e-01, -1.82993397e-01,  1.81020930e-01,\n",
       "          1.50071248e-01,  1.54021829e-02,  3.55644971e-02,\n",
       "          2.19199821e-01, -3.06687057e-02, -2.12688312e-01,\n",
       "          2.30652317e-01,  1.09585300e-01, -4.81246114e-02,\n",
       "          9.79584903e-02, -1.44602776e-01, -1.31996110e-01,\n",
       "         -9.66711938e-02, -1.01906851e-01,  2.19168529e-01,\n",
       "          1.24274984e-01, -2.60927379e-02, -4.40360457e-02,\n",
       "         -1.76647976e-01, -6.68861121e-02, -1.54381230e-01,\n",
       "         -1.15995228e-01,  4.57877368e-02,  8.77382308e-02,\n",
       "         -1.17367044e-01,  8.93470794e-02, -2.22226873e-01,\n",
       "         -1.53988600e-04, -1.82377756e-01, -6.20600134e-02,\n",
       "         -2.24550828e-01,  2.18593225e-01, -1.78050518e-01,\n",
       "         -2.17485011e-01,  2.33387753e-01,  1.79188475e-01,\n",
       "          1.94453374e-01,  2.37100109e-01,  3.87431234e-02,\n",
       "          5.86895794e-02, -1.76074386e-01,  2.04106256e-01,\n",
       "         -6.52769208e-02, -1.60353065e-01, -6.85796142e-03,\n",
       "         -1.93586782e-01, -1.95775926e-02, -6.36729747e-02,\n",
       "          1.15722582e-01,  1.07722417e-01, -1.35371983e-01,\n",
       "         -3.67757827e-02, -9.03346688e-02, -1.42799467e-01,\n",
       "         -1.62090778e-01],\n",
       "        [ 2.17717156e-01, -8.28039646e-02, -1.88813746e-01,\n",
       "         -4.57310230e-02,  1.97640762e-01, -1.35834873e-01,\n",
       "         -3.05355638e-02,  2.53190547e-02, -2.26580292e-01,\n",
       "          9.09621567e-02, -1.72147349e-01, -2.09394574e-01,\n",
       "         -1.65415287e-01,  1.94137916e-01, -9.61877257e-02,\n",
       "         -7.66522586e-02,  1.91312984e-01, -2.55887508e-02,\n",
       "          1.65491387e-01,  1.17886886e-01, -1.69430435e-01,\n",
       "         -1.53337270e-02,  1.14062205e-01, -2.35175848e-01,\n",
       "          9.17162448e-02,  2.88482755e-02, -9.77834761e-02,\n",
       "         -1.64721802e-01, -9.18633342e-02, -1.54029012e-01,\n",
       "         -8.41927677e-02, -5.01578897e-02,  2.14321002e-01,\n",
       "          1.96851239e-01,  1.07689306e-01,  1.27500162e-01,\n",
       "          1.67425260e-01, -8.49939138e-02,  2.23933652e-01,\n",
       "         -1.53758973e-01, -1.14425927e-01, -2.27940083e-01,\n",
       "         -8.96266401e-02,  1.97773144e-01, -1.03557482e-01,\n",
       "         -9.15490687e-02, -1.55261427e-01,  1.73964188e-01,\n",
       "          8.97324830e-02,  1.65425822e-01,  2.27428511e-01,\n",
       "         -1.28824562e-01,  1.63902685e-01,  1.61321267e-01,\n",
       "         -2.08213478e-02, -2.29404286e-01, -1.23226948e-01,\n",
       "          2.00567916e-01,  1.86297551e-01,  2.35840335e-01,\n",
       "         -2.03428775e-01, -1.28435254e-01, -4.75290865e-02,\n",
       "          1.39837131e-01, -5.06394207e-02,  2.09831998e-01,\n",
       "         -2.10860148e-01, -2.16689780e-01,  2.32032284e-01,\n",
       "         -2.10462362e-02,  2.25880995e-01, -4.16042805e-02,\n",
       "         -1.90927714e-01, -2.01208457e-01,  8.94773155e-02,\n",
       "          2.28919163e-01,  1.86338529e-01,  2.12945357e-01,\n",
       "         -2.20692858e-01,  6.00886196e-02, -9.37580615e-02,\n",
       "          5.97584695e-02, -1.71698645e-01, -1.27952546e-02,\n",
       "          8.52069110e-02, -4.99628633e-02,  2.08935589e-02,\n",
       "          1.84978798e-01, -4.26788777e-02, -4.03700322e-02,\n",
       "          2.04159424e-01,  1.27073452e-01,  3.14204246e-02,\n",
       "         -5.91386110e-02,  1.40271708e-01,  2.30273321e-01,\n",
       "         -5.25864214e-02, -1.79560140e-01,  7.58592635e-02,\n",
       "          2.32665136e-01],\n",
       "        [-1.69238597e-01, -1.21254079e-01, -2.10072905e-01,\n",
       "         -1.29545480e-01,  1.03416607e-01,  4.45884466e-03,\n",
       "         -2.20622569e-02, -1.04406342e-01, -6.05113953e-02,\n",
       "         -2.10225135e-01, -1.17846243e-01, -2.90481597e-02,\n",
       "          8.21242183e-02,  1.19649157e-01, -5.48561662e-02,\n",
       "         -1.41984746e-01, -1.93331629e-01, -1.09425366e-01,\n",
       "         -2.37813190e-01,  1.98395401e-02, -9.28051919e-02,\n",
       "          2.09780827e-01, -1.91005751e-01,  2.38995060e-01,\n",
       "         -6.50854260e-02,  1.42690390e-02,  1.39506534e-01,\n",
       "         -3.78058106e-02, -1.75690487e-01,  1.27652213e-01,\n",
       "         -1.26560003e-01, -1.75320387e-01, -1.14942968e-01,\n",
       "          6.89132959e-02, -3.20339054e-02,  3.65303010e-02,\n",
       "          8.42512995e-02, -2.08679765e-01,  1.10541627e-01,\n",
       "          2.04183087e-01,  4.51679379e-02,  4.58477885e-02,\n",
       "          3.51261050e-02,  6.26022965e-02, -9.98203456e-02,\n",
       "          1.06954262e-01,  2.23713264e-01,  1.98422059e-01,\n",
       "         -1.01257816e-01, -1.05809897e-01,  8.77567679e-02,\n",
       "         -1.61951780e-02, -1.82102144e-01,  3.35082561e-02,\n",
       "         -2.13919640e-01,  1.62995651e-01, -2.24160314e-01,\n",
       "         -6.85303062e-02, -9.49871689e-02, -4.53829765e-03,\n",
       "          1.08215362e-02, -1.88208476e-01,  2.10388944e-01,\n",
       "          6.89681917e-02,  1.84092775e-01, -7.77289718e-02,\n",
       "          2.25767121e-01,  1.99590191e-01,  1.26223281e-01,\n",
       "         -1.65207773e-01,  1.19284540e-02,  1.94992408e-01,\n",
       "         -4.98168468e-02, -2.29880288e-01,  1.88782409e-01,\n",
       "          1.34822950e-01, -3.60803455e-02, -8.13783407e-02,\n",
       "          3.23572904e-02, -5.36100715e-02, -2.08284706e-02,\n",
       "          8.84785801e-02, -1.84989974e-01,  1.82557449e-01,\n",
       "         -1.71795875e-01,  6.30815327e-03, -2.15803236e-02,\n",
       "         -7.82535374e-02,  2.19335273e-01,  5.92528731e-02,\n",
       "         -1.87398672e-01,  5.17226905e-02,  1.08067617e-01,\n",
       "         -2.16781199e-02, -1.39420629e-02,  9.25952941e-02,\n",
       "          4.42539155e-03, -1.98739976e-01, -1.46353945e-01,\n",
       "          2.23324046e-01],\n",
       "        [ 1.24941930e-01,  2.32866272e-01, -9.32575762e-03,\n",
       "         -2.77535617e-02,  1.11107811e-01, -4.18351591e-03,\n",
       "         -1.21697545e-01,  2.40504593e-02, -1.86598033e-01,\n",
       "         -8.28341693e-02,  4.56430167e-02,  2.33183131e-01,\n",
       "         -1.08594924e-01,  1.01664171e-01,  5.67907840e-02,\n",
       "          1.38806835e-01,  1.98382869e-01,  1.18342265e-01,\n",
       "          1.98898688e-01, -8.40247571e-02,  1.58285245e-01,\n",
       "          1.60705283e-01,  2.10546002e-01,  1.86278075e-02,\n",
       "         -7.47416914e-03,  3.22442800e-02,  1.40637949e-01,\n",
       "         -7.69231468e-02, -3.16400826e-02,  9.00861174e-02,\n",
       "         -7.11484998e-02,  1.00347087e-01, -1.58891082e-04,\n",
       "         -1.52292609e-01, -9.43686813e-02, -2.09399134e-01,\n",
       "         -7.78593123e-02, -1.24087028e-01, -3.72442007e-02,\n",
       "          1.72756657e-01,  2.04643235e-01,  1.19231299e-01,\n",
       "          1.12885818e-01, -5.00685275e-02, -1.65908456e-01,\n",
       "          2.10424617e-01,  8.30297619e-02,  2.06616059e-01,\n",
       "         -8.08441490e-02,  1.29998550e-01,  9.76507515e-02,\n",
       "         -1.16854906e-01, -8.74979645e-02, -2.21814647e-01,\n",
       "         -2.08214030e-01, -1.57299161e-01,  1.28576711e-01,\n",
       "          9.12037641e-02, -3.79610062e-02,  1.33591339e-01,\n",
       "         -1.27787232e-01,  1.20182827e-01, -1.25710070e-01,\n",
       "         -2.31546313e-01, -1.05230689e-01,  1.90683499e-01,\n",
       "         -3.24639231e-02, -1.29800290e-01, -1.14685982e-01,\n",
       "          2.34136924e-01,  2.04911098e-01, -5.46202064e-02,\n",
       "         -1.31552994e-01,  3.11737806e-02,  1.73603132e-01,\n",
       "          2.15449288e-01, -1.97991654e-01,  5.31991571e-02,\n",
       "         -2.64879763e-02, -3.21720093e-02, -1.40429974e-01,\n",
       "          7.99656659e-02,  1.10633329e-01,  2.52799541e-02,\n",
       "          2.94875652e-02,  1.09223738e-01, -1.16484463e-02,\n",
       "          1.11307651e-02,  1.62317827e-01,  3.57513279e-02,\n",
       "          7.50506371e-02, -1.02608785e-01, -1.20698683e-01,\n",
       "          3.70268971e-02,  2.07259521e-01,  2.20584422e-02,\n",
       "         -1.55671716e-01,  4.97011989e-02, -4.13042158e-02,\n",
       "         -1.18735731e-01]], dtype=float32)>,\n",
       " <tf.Variable 'dense_7/bias:0' shape=(100,) dtype=float32, numpy=\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       dtype=float32)>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer.variables\n",
    "# x * w + b  :w --dense_7/kernel:0', b --'dense_7/bias:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'dense_7/kernel:0' shape=(5, 100) dtype=float32, numpy=\n",
       " array([[ 1.10843793e-01, -1.22447059e-01,  2.52075344e-02,\n",
       "          9.04147476e-02, -6.48622960e-02, -1.75273016e-01,\n",
       "         -1.37443960e-01,  2.02868894e-01,  2.89248973e-02,\n",
       "          3.64692062e-02, -1.52013123e-01,  2.80783623e-02,\n",
       "          1.78069845e-01, -3.23069692e-02,  8.88404399e-02,\n",
       "          2.59214789e-02,  3.39988619e-02, -1.30343437e-02,\n",
       "          2.06639931e-01,  6.25833720e-02,  8.13232362e-03,\n",
       "          2.09118798e-01, -1.62215769e-01,  1.30225345e-01,\n",
       "          1.11125365e-01, -1.21910721e-02, -4.06540930e-02,\n",
       "          2.03676373e-02,  2.20266595e-01, -1.80565566e-02,\n",
       "         -2.16052830e-02, -1.69636518e-01,  1.45049497e-01,\n",
       "         -2.34577194e-01,  1.09925315e-01, -1.90073058e-01,\n",
       "         -4.37143892e-02, -2.23394841e-01, -6.42518997e-02,\n",
       "          6.69075996e-02, -8.79259706e-02,  2.08262458e-01,\n",
       "         -2.31247887e-01,  2.57948786e-02, -9.18564349e-02,\n",
       "         -9.18298811e-02,  2.00986192e-01,  4.91460413e-02,\n",
       "          8.26503187e-02,  1.29991338e-01, -8.56688321e-02,\n",
       "         -2.27528423e-01,  1.07959792e-01,  2.85352319e-02,\n",
       "         -4.15598750e-02,  1.06466964e-01, -2.27778792e-01,\n",
       "         -8.82050097e-02,  2.19237193e-01,  1.47422954e-01,\n",
       "          8.95149857e-02,  2.27172032e-01, -1.42482698e-01,\n",
       "          1.77204505e-01, -3.95187289e-02,  1.68357030e-01,\n",
       "         -4.10866737e-02,  2.44677514e-02, -1.64605081e-01,\n",
       "         -8.65355134e-02,  1.88850626e-01, -2.31448799e-01,\n",
       "         -1.23012714e-01,  2.28242084e-01,  1.30742341e-02,\n",
       "         -6.63207471e-02, -1.28714114e-01,  4.85761613e-02,\n",
       "         -7.51079172e-02,  7.58397728e-02,  6.20642304e-03,\n",
       "          1.72286019e-01, -1.04165375e-01,  1.24849245e-01,\n",
       "         -7.45871216e-02,  1.34786025e-01,  2.28142455e-01,\n",
       "          1.56275377e-01, -2.12182611e-01, -2.22305954e-03,\n",
       "          2.12509140e-01,  1.37818739e-01,  1.42191932e-01,\n",
       "         -7.17578828e-02, -1.23335347e-01,  2.47346610e-02,\n",
       "         -3.04381549e-03, -1.84358597e-01, -6.50027394e-03,\n",
       "          2.02306882e-01],\n",
       "        [ 1.50109217e-01,  2.60302871e-02,  2.30452314e-01,\n",
       "          3.25347632e-02,  5.33261448e-02,  1.54889092e-01,\n",
       "         -4.42259461e-02,  8.17080885e-02,  1.57935992e-01,\n",
       "         -1.12476647e-01, -1.12285107e-01,  1.42938808e-01,\n",
       "          5.09825349e-03,  8.25136453e-02, -3.72909307e-02,\n",
       "         -4.77517694e-02, -8.50712508e-02,  1.81189343e-01,\n",
       "         -2.73434520e-02,  2.20017284e-02,  5.81039637e-02,\n",
       "          6.86147660e-02, -2.01499343e-01, -1.33144051e-01,\n",
       "          1.17997959e-01, -1.88388690e-01,  2.29842439e-01,\n",
       "         -2.38078609e-01,  1.12121806e-01, -8.03190470e-03,\n",
       "          1.57892063e-01,  1.28679052e-01,  1.13071367e-01,\n",
       "          1.73953637e-01,  1.05934665e-01,  2.28262886e-01,\n",
       "          2.00669363e-01, -2.20045358e-01, -1.65049851e-01,\n",
       "          2.06436828e-01,  1.22708872e-01, -1.50521666e-01,\n",
       "          1.06381670e-01, -1.82993397e-01,  1.81020930e-01,\n",
       "          1.50071248e-01,  1.54021829e-02,  3.55644971e-02,\n",
       "          2.19199821e-01, -3.06687057e-02, -2.12688312e-01,\n",
       "          2.30652317e-01,  1.09585300e-01, -4.81246114e-02,\n",
       "          9.79584903e-02, -1.44602776e-01, -1.31996110e-01,\n",
       "         -9.66711938e-02, -1.01906851e-01,  2.19168529e-01,\n",
       "          1.24274984e-01, -2.60927379e-02, -4.40360457e-02,\n",
       "         -1.76647976e-01, -6.68861121e-02, -1.54381230e-01,\n",
       "         -1.15995228e-01,  4.57877368e-02,  8.77382308e-02,\n",
       "         -1.17367044e-01,  8.93470794e-02, -2.22226873e-01,\n",
       "         -1.53988600e-04, -1.82377756e-01, -6.20600134e-02,\n",
       "         -2.24550828e-01,  2.18593225e-01, -1.78050518e-01,\n",
       "         -2.17485011e-01,  2.33387753e-01,  1.79188475e-01,\n",
       "          1.94453374e-01,  2.37100109e-01,  3.87431234e-02,\n",
       "          5.86895794e-02, -1.76074386e-01,  2.04106256e-01,\n",
       "         -6.52769208e-02, -1.60353065e-01, -6.85796142e-03,\n",
       "         -1.93586782e-01, -1.95775926e-02, -6.36729747e-02,\n",
       "          1.15722582e-01,  1.07722417e-01, -1.35371983e-01,\n",
       "         -3.67757827e-02, -9.03346688e-02, -1.42799467e-01,\n",
       "         -1.62090778e-01],\n",
       "        [ 2.17717156e-01, -8.28039646e-02, -1.88813746e-01,\n",
       "         -4.57310230e-02,  1.97640762e-01, -1.35834873e-01,\n",
       "         -3.05355638e-02,  2.53190547e-02, -2.26580292e-01,\n",
       "          9.09621567e-02, -1.72147349e-01, -2.09394574e-01,\n",
       "         -1.65415287e-01,  1.94137916e-01, -9.61877257e-02,\n",
       "         -7.66522586e-02,  1.91312984e-01, -2.55887508e-02,\n",
       "          1.65491387e-01,  1.17886886e-01, -1.69430435e-01,\n",
       "         -1.53337270e-02,  1.14062205e-01, -2.35175848e-01,\n",
       "          9.17162448e-02,  2.88482755e-02, -9.77834761e-02,\n",
       "         -1.64721802e-01, -9.18633342e-02, -1.54029012e-01,\n",
       "         -8.41927677e-02, -5.01578897e-02,  2.14321002e-01,\n",
       "          1.96851239e-01,  1.07689306e-01,  1.27500162e-01,\n",
       "          1.67425260e-01, -8.49939138e-02,  2.23933652e-01,\n",
       "         -1.53758973e-01, -1.14425927e-01, -2.27940083e-01,\n",
       "         -8.96266401e-02,  1.97773144e-01, -1.03557482e-01,\n",
       "         -9.15490687e-02, -1.55261427e-01,  1.73964188e-01,\n",
       "          8.97324830e-02,  1.65425822e-01,  2.27428511e-01,\n",
       "         -1.28824562e-01,  1.63902685e-01,  1.61321267e-01,\n",
       "         -2.08213478e-02, -2.29404286e-01, -1.23226948e-01,\n",
       "          2.00567916e-01,  1.86297551e-01,  2.35840335e-01,\n",
       "         -2.03428775e-01, -1.28435254e-01, -4.75290865e-02,\n",
       "          1.39837131e-01, -5.06394207e-02,  2.09831998e-01,\n",
       "         -2.10860148e-01, -2.16689780e-01,  2.32032284e-01,\n",
       "         -2.10462362e-02,  2.25880995e-01, -4.16042805e-02,\n",
       "         -1.90927714e-01, -2.01208457e-01,  8.94773155e-02,\n",
       "          2.28919163e-01,  1.86338529e-01,  2.12945357e-01,\n",
       "         -2.20692858e-01,  6.00886196e-02, -9.37580615e-02,\n",
       "          5.97584695e-02, -1.71698645e-01, -1.27952546e-02,\n",
       "          8.52069110e-02, -4.99628633e-02,  2.08935589e-02,\n",
       "          1.84978798e-01, -4.26788777e-02, -4.03700322e-02,\n",
       "          2.04159424e-01,  1.27073452e-01,  3.14204246e-02,\n",
       "         -5.91386110e-02,  1.40271708e-01,  2.30273321e-01,\n",
       "         -5.25864214e-02, -1.79560140e-01,  7.58592635e-02,\n",
       "          2.32665136e-01],\n",
       "        [-1.69238597e-01, -1.21254079e-01, -2.10072905e-01,\n",
       "         -1.29545480e-01,  1.03416607e-01,  4.45884466e-03,\n",
       "         -2.20622569e-02, -1.04406342e-01, -6.05113953e-02,\n",
       "         -2.10225135e-01, -1.17846243e-01, -2.90481597e-02,\n",
       "          8.21242183e-02,  1.19649157e-01, -5.48561662e-02,\n",
       "         -1.41984746e-01, -1.93331629e-01, -1.09425366e-01,\n",
       "         -2.37813190e-01,  1.98395401e-02, -9.28051919e-02,\n",
       "          2.09780827e-01, -1.91005751e-01,  2.38995060e-01,\n",
       "         -6.50854260e-02,  1.42690390e-02,  1.39506534e-01,\n",
       "         -3.78058106e-02, -1.75690487e-01,  1.27652213e-01,\n",
       "         -1.26560003e-01, -1.75320387e-01, -1.14942968e-01,\n",
       "          6.89132959e-02, -3.20339054e-02,  3.65303010e-02,\n",
       "          8.42512995e-02, -2.08679765e-01,  1.10541627e-01,\n",
       "          2.04183087e-01,  4.51679379e-02,  4.58477885e-02,\n",
       "          3.51261050e-02,  6.26022965e-02, -9.98203456e-02,\n",
       "          1.06954262e-01,  2.23713264e-01,  1.98422059e-01,\n",
       "         -1.01257816e-01, -1.05809897e-01,  8.77567679e-02,\n",
       "         -1.61951780e-02, -1.82102144e-01,  3.35082561e-02,\n",
       "         -2.13919640e-01,  1.62995651e-01, -2.24160314e-01,\n",
       "         -6.85303062e-02, -9.49871689e-02, -4.53829765e-03,\n",
       "          1.08215362e-02, -1.88208476e-01,  2.10388944e-01,\n",
       "          6.89681917e-02,  1.84092775e-01, -7.77289718e-02,\n",
       "          2.25767121e-01,  1.99590191e-01,  1.26223281e-01,\n",
       "         -1.65207773e-01,  1.19284540e-02,  1.94992408e-01,\n",
       "         -4.98168468e-02, -2.29880288e-01,  1.88782409e-01,\n",
       "          1.34822950e-01, -3.60803455e-02, -8.13783407e-02,\n",
       "          3.23572904e-02, -5.36100715e-02, -2.08284706e-02,\n",
       "          8.84785801e-02, -1.84989974e-01,  1.82557449e-01,\n",
       "         -1.71795875e-01,  6.30815327e-03, -2.15803236e-02,\n",
       "         -7.82535374e-02,  2.19335273e-01,  5.92528731e-02,\n",
       "         -1.87398672e-01,  5.17226905e-02,  1.08067617e-01,\n",
       "         -2.16781199e-02, -1.39420629e-02,  9.25952941e-02,\n",
       "          4.42539155e-03, -1.98739976e-01, -1.46353945e-01,\n",
       "          2.23324046e-01],\n",
       "        [ 1.24941930e-01,  2.32866272e-01, -9.32575762e-03,\n",
       "         -2.77535617e-02,  1.11107811e-01, -4.18351591e-03,\n",
       "         -1.21697545e-01,  2.40504593e-02, -1.86598033e-01,\n",
       "         -8.28341693e-02,  4.56430167e-02,  2.33183131e-01,\n",
       "         -1.08594924e-01,  1.01664171e-01,  5.67907840e-02,\n",
       "          1.38806835e-01,  1.98382869e-01,  1.18342265e-01,\n",
       "          1.98898688e-01, -8.40247571e-02,  1.58285245e-01,\n",
       "          1.60705283e-01,  2.10546002e-01,  1.86278075e-02,\n",
       "         -7.47416914e-03,  3.22442800e-02,  1.40637949e-01,\n",
       "         -7.69231468e-02, -3.16400826e-02,  9.00861174e-02,\n",
       "         -7.11484998e-02,  1.00347087e-01, -1.58891082e-04,\n",
       "         -1.52292609e-01, -9.43686813e-02, -2.09399134e-01,\n",
       "         -7.78593123e-02, -1.24087028e-01, -3.72442007e-02,\n",
       "          1.72756657e-01,  2.04643235e-01,  1.19231299e-01,\n",
       "          1.12885818e-01, -5.00685275e-02, -1.65908456e-01,\n",
       "          2.10424617e-01,  8.30297619e-02,  2.06616059e-01,\n",
       "         -8.08441490e-02,  1.29998550e-01,  9.76507515e-02,\n",
       "         -1.16854906e-01, -8.74979645e-02, -2.21814647e-01,\n",
       "         -2.08214030e-01, -1.57299161e-01,  1.28576711e-01,\n",
       "          9.12037641e-02, -3.79610062e-02,  1.33591339e-01,\n",
       "         -1.27787232e-01,  1.20182827e-01, -1.25710070e-01,\n",
       "         -2.31546313e-01, -1.05230689e-01,  1.90683499e-01,\n",
       "         -3.24639231e-02, -1.29800290e-01, -1.14685982e-01,\n",
       "          2.34136924e-01,  2.04911098e-01, -5.46202064e-02,\n",
       "         -1.31552994e-01,  3.11737806e-02,  1.73603132e-01,\n",
       "          2.15449288e-01, -1.97991654e-01,  5.31991571e-02,\n",
       "         -2.64879763e-02, -3.21720093e-02, -1.40429974e-01,\n",
       "          7.99656659e-02,  1.10633329e-01,  2.52799541e-02,\n",
       "          2.94875652e-02,  1.09223738e-01, -1.16484463e-02,\n",
       "          1.11307651e-02,  1.62317827e-01,  3.57513279e-02,\n",
       "          7.50506371e-02, -1.02608785e-01, -1.20698683e-01,\n",
       "          3.70268971e-02,  2.07259521e-01,  2.20584422e-02,\n",
       "         -1.55671716e-01,  4.97011989e-02, -4.13042158e-02,\n",
       "         -1.18735731e-01]], dtype=float32)>,\n",
       " <tf.Variable 'dense_7/bias:0' shape=(100,) dtype=float32, numpy=\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       dtype=float32)>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer.trainable_variables #可训练变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on Dense in module tensorflow.python.keras.layers.core object:\n",
      "\n",
      "class Dense(tensorflow.python.keras.engine.base_layer.Layer)\n",
      " |  Dense(units, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None, **kwargs)\n",
      " |  \n",
      " |  Just your regular densely-connected NN layer.\n",
      " |  \n",
      " |  `Dense` implements the operation:\n",
      " |  `output = activation(dot(input, kernel) + bias)`\n",
      " |  where `activation` is the element-wise activation function\n",
      " |  passed as the `activation` argument, `kernel` is a weights matrix\n",
      " |  created by the layer, and `bias` is a bias vector created by the layer\n",
      " |  (only applicable if `use_bias` is `True`).\n",
      " |  \n",
      " |  Note: If the input to the layer has a rank greater than 2, then\n",
      " |  it is flattened prior to the initial dot product with `kernel`.\n",
      " |  \n",
      " |  Example:\n",
      " |  \n",
      " |  ```python\n",
      " |  # as first layer in a sequential model:\n",
      " |  model = Sequential()\n",
      " |  model.add(Dense(32, input_shape=(16,)))\n",
      " |  # now the model will take as input arrays of shape (*, 16)\n",
      " |  # and output arrays of shape (*, 32)\n",
      " |  \n",
      " |  # after the first layer, you don't need to specify\n",
      " |  # the size of the input anymore:\n",
      " |  model.add(Dense(32))\n",
      " |  ```\n",
      " |  \n",
      " |  Arguments:\n",
      " |    units: Positive integer, dimensionality of the output space.\n",
      " |    activation: Activation function to use.\n",
      " |      If you don't specify anything, no activation is applied\n",
      " |      (ie. \"linear\" activation: `a(x) = x`).\n",
      " |    use_bias: Boolean, whether the layer uses a bias vector.\n",
      " |    kernel_initializer: Initializer for the `kernel` weights matrix.\n",
      " |    bias_initializer: Initializer for the bias vector.\n",
      " |    kernel_regularizer: Regularizer function applied to\n",
      " |      the `kernel` weights matrix.\n",
      " |    bias_regularizer: Regularizer function applied to the bias vector.\n",
      " |    activity_regularizer: Regularizer function applied to\n",
      " |      the output of the layer (its \"activation\")..\n",
      " |    kernel_constraint: Constraint function applied to\n",
      " |      the `kernel` weights matrix.\n",
      " |    bias_constraint: Constraint function applied to the bias vector.\n",
      " |  \n",
      " |  Input shape:\n",
      " |    N-D tensor with shape: `(batch_size, ..., input_dim)`.\n",
      " |    The most common situation would be\n",
      " |    a 2D input with shape `(batch_size, input_dim)`.\n",
      " |  \n",
      " |  Output shape:\n",
      " |    N-D tensor with shape: `(batch_size, ..., units)`.\n",
      " |    For instance, for a 2D input with shape `(batch_size, input_dim)`,\n",
      " |    the output would have shape `(batch_size, units)`.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Dense\n",
      " |      tensorflow.python.keras.engine.base_layer.Layer\n",
      " |      tensorflow.python.module.module.Module\n",
      " |      tensorflow.python.training.tracking.tracking.AutoTrackable\n",
      " |      tensorflow.python.training.tracking.base.Trackable\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, units, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None, **kwargs)\n",
      " |  \n",
      " |  build(self, input_shape)\n",
      " |      Creates the variables of the layer (optional, for subclass implementers).\n",
      " |      \n",
      " |      This is a method that implementers of subclasses of `Layer` or `Model`\n",
      " |      can override if they need a state-creation step in-between\n",
      " |      layer instantiation and layer call.\n",
      " |      \n",
      " |      This is typically used to create the weights of `Layer` subclasses.\n",
      " |      \n",
      " |      Arguments:\n",
      " |        input_shape: Instance of `TensorShape`, or list of instances of\n",
      " |          `TensorShape` if the layer expects a list of inputs\n",
      " |          (one instance per input).\n",
      " |  \n",
      " |  call(self, inputs)\n",
      " |      This is where the layer's logic lives.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          inputs: Input tensor, or list/tuple of input tensors.\n",
      " |          **kwargs: Additional keyword arguments.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A tensor or list/tuple of tensors.\n",
      " |  \n",
      " |  compute_output_shape(self, input_shape)\n",
      " |      Computes the output shape of the layer.\n",
      " |      \n",
      " |      If the layer has not been built, this method will call `build` on the\n",
      " |      layer. This assumes that the layer will later be used with inputs that\n",
      " |      match the input shape provided here.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          input_shape: Shape tuple (tuple of integers)\n",
      " |              or list of shape tuples (one per output tensor of the layer).\n",
      " |              Shape tuples can include None for free dimensions,\n",
      " |              instead of an integer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          An input shape tuple.\n",
      " |  \n",
      " |  get_config(self)\n",
      " |      Returns the config of the layer.\n",
      " |      \n",
      " |      A layer config is a Python dictionary (serializable)\n",
      " |      containing the configuration of a layer.\n",
      " |      The same layer can be reinstantiated later\n",
      " |      (without its trained weights) from this configuration.\n",
      " |      \n",
      " |      The config of a layer does not include connectivity\n",
      " |      information, nor the layer class name. These are handled\n",
      " |      by `Network` (one layer of abstraction above).\n",
      " |      \n",
      " |      Returns:\n",
      " |          Python dictionary.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from tensorflow.python.keras.engine.base_layer.Layer:\n",
      " |  \n",
      " |  __call__(self, inputs, *args, **kwargs)\n",
      " |      Wraps `call`, applying pre- and post-processing steps.\n",
      " |      \n",
      " |      Arguments:\n",
      " |        inputs: input tensor(s).\n",
      " |        *args: additional positional arguments to be passed to `self.call`.\n",
      " |        **kwargs: additional keyword arguments to be passed to `self.call`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Output tensor(s).\n",
      " |      \n",
      " |      Note:\n",
      " |        - The following optional keyword arguments are reserved for specific uses:\n",
      " |          * `training`: Boolean scalar tensor of Python boolean indicating\n",
      " |            whether the `call` is meant for training or inference.\n",
      " |          * `mask`: Boolean input mask.\n",
      " |        - If the layer's `call` method takes a `mask` argument (as some Keras\n",
      " |          layers do), its default value will be set to the mask generated\n",
      " |          for `inputs` by the previous layer (if `input` did come from\n",
      " |          a layer that generated a corresponding mask, i.e. if it came from\n",
      " |          a Keras layer with masking support.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: if the layer's `call` method returns None (an invalid value).\n",
      " |  \n",
      " |  __delattr__(self, name)\n",
      " |      Implement delattr(self, name).\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __setattr__(self, name, value)\n",
      " |      Support self.foo = trackable syntax.\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  add_loss(self, losses, inputs=None)\n",
      " |      Add loss tensor(s), potentially dependent on layer inputs.\n",
      " |      \n",
      " |      Some losses (for instance, activity regularization losses) may be dependent\n",
      " |      on the inputs passed when calling a layer. Hence, when reusing the same\n",
      " |      layer on different inputs `a` and `b`, some entries in `layer.losses` may\n",
      " |      be dependent on `a` and some on `b`. This method automatically keeps track\n",
      " |      of dependencies.\n",
      " |      \n",
      " |      This method can be used inside a subclassed layer or model's `call`\n",
      " |      function, in which case `losses` should be a Tensor or list of Tensors.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      class MyLayer(tf.keras.layers.Layer):\n",
      " |        def call(inputs, self):\n",
      " |          self.add_loss(tf.abs(tf.reduce_mean(inputs)), inputs=True)\n",
      " |          return inputs\n",
      " |      ```\n",
      " |      \n",
      " |      This method can also be called directly on a Functional Model during\n",
      " |      construction. In this case, any loss Tensors passed to this Model must\n",
      " |      be symbolic and be able to be traced back to the model's `Input`s. These\n",
      " |      losses become part of the model's topology and are tracked in `get_config`.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      inputs = tf.keras.Input(shape=(10,))\n",
      " |      x = tf.keras.layers.Dense(10)(inputs)\n",
      " |      outputs = tf.keras.layers.Dense(1)(x)\n",
      " |      model = tf.keras.Model(inputs, outputs)\n",
      " |      # Actvity regularization.\n",
      " |      model.add_loss(tf.abs(tf.reduce_mean(x)))\n",
      " |      ```\n",
      " |      \n",
      " |      If this is not the case for your loss (if, for example, your loss references\n",
      " |      a `Variable` of one of the model's layers), you can wrap your loss in a\n",
      " |      zero-argument lambda. These losses are not tracked as part of the model's\n",
      " |      topology since they can't be serialized.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      inputs = tf.keras.Input(shape=(10,))\n",
      " |      x = tf.keras.layers.Dense(10)(inputs)\n",
      " |      outputs = tf.keras.layers.Dense(1)(x)\n",
      " |      model = tf.keras.Model(inputs, outputs)\n",
      " |      # Weight regularization.\n",
      " |      model.add_loss(lambda: tf.reduce_mean(x.kernel))\n",
      " |      ```\n",
      " |      \n",
      " |      The `get_losses_for` method allows to retrieve the losses relevant to a\n",
      " |      specific set of inputs.\n",
      " |      \n",
      " |      Arguments:\n",
      " |        losses: Loss tensor, or list/tuple of tensors. Rather than tensors, losses\n",
      " |          may also be zero-argument callables which create a loss tensor.\n",
      " |        inputs: Ignored when executing eagerly. If anything other than None is\n",
      " |          passed, it signals the losses are conditional on some of the layer's\n",
      " |          inputs, and thus they should only be run where these inputs are\n",
      " |          available. This is the case for activity regularization losses, for\n",
      " |          instance. If `None` is passed, the losses are assumed\n",
      " |          to be unconditional, and will apply across all dataflows of the layer\n",
      " |          (e.g. weight regularization losses).\n",
      " |  \n",
      " |  add_metric(self, value, aggregation=None, name=None)\n",
      " |      Adds metric tensor to the layer.\n",
      " |      \n",
      " |      Args:\n",
      " |        value: Metric tensor.\n",
      " |        aggregation: Sample-wise metric reduction function. If `aggregation=None`,\n",
      " |          it indicates that the metric tensor provided has been aggregated\n",
      " |          already. eg, `bin_acc = BinaryAccuracy(name='acc')` followed by\n",
      " |          `model.add_metric(bin_acc(y_true, y_pred))`. If aggregation='mean', the\n",
      " |          given metric tensor will be sample-wise reduced using `mean` function.\n",
      " |          eg, `model.add_metric(tf.reduce_sum(outputs), name='output_mean',\n",
      " |          aggregation='mean')`.\n",
      " |        name: String metric name.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: If `aggregation` is anything other than None or `mean`.\n",
      " |  \n",
      " |  add_update(self, updates, inputs=None)\n",
      " |      Add update op(s), potentially dependent on layer inputs. (deprecated arguments)\n",
      " |      \n",
      " |      Warning: SOME ARGUMENTS ARE DEPRECATED: `(inputs)`. They will be removed in a future version.\n",
      " |      Instructions for updating:\n",
      " |      `inputs` is now automatically inferred\n",
      " |      \n",
      " |      Weight updates (for instance, the updates of the moving mean and variance\n",
      " |      in a BatchNormalization layer) may be dependent on the inputs passed\n",
      " |      when calling a layer. Hence, when reusing the same layer on\n",
      " |      different inputs `a` and `b`, some entries in `layer.updates` may be\n",
      " |      dependent on `a` and some on `b`. This method automatically keeps track\n",
      " |      of dependencies.\n",
      " |      \n",
      " |      The `get_updates_for` method allows to retrieve the updates relevant to a\n",
      " |      specific set of inputs.\n",
      " |      \n",
      " |      This call is ignored when eager execution is enabled (in that case, variable\n",
      " |      updates are run on the fly and thus do not need to be tracked for later\n",
      " |      execution).\n",
      " |      \n",
      " |      Arguments:\n",
      " |        updates: Update op, or list/tuple of update ops, or zero-arg callable\n",
      " |          that returns an update op. A zero-arg callable should be passed in\n",
      " |          order to disable running the updates by setting `trainable=False`\n",
      " |          on this Layer, when executing in Eager mode.\n",
      " |        inputs: Deprecated, will be automatically inferred.\n",
      " |  \n",
      " |  add_variable(self, *args, **kwargs)\n",
      " |      Deprecated, do NOT use! Alias for `add_weight`. (deprecated)\n",
      " |      \n",
      " |      Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.\n",
      " |      Instructions for updating:\n",
      " |      Please use `layer.add_weight` method instead.\n",
      " |  \n",
      " |  add_weight(self, name=None, shape=None, dtype=None, initializer=None, regularizer=None, trainable=None, constraint=None, partitioner=None, use_resource=None, synchronization=<VariableSynchronization.AUTO: 0>, aggregation=<VariableAggregation.NONE: 0>, **kwargs)\n",
      " |      Adds a new variable to the layer.\n",
      " |      \n",
      " |      Arguments:\n",
      " |        name: Variable name.\n",
      " |        shape: Variable shape. Defaults to scalar if unspecified.\n",
      " |        dtype: The type of the variable. Defaults to `self.dtype` or `float32`.\n",
      " |        initializer: Initializer instance (callable).\n",
      " |        regularizer: Regularizer instance (callable).\n",
      " |        trainable: Boolean, whether the variable should be part of the layer's\n",
      " |          \"trainable_variables\" (e.g. variables, biases)\n",
      " |          or \"non_trainable_variables\" (e.g. BatchNorm mean and variance).\n",
      " |          Note that `trainable` cannot be `True` if `synchronization`\n",
      " |          is set to `ON_READ`.\n",
      " |        constraint: Constraint instance (callable).\n",
      " |        partitioner: Partitioner to be passed to the `Trackable` API.\n",
      " |        use_resource: Whether to use `ResourceVariable`.\n",
      " |        synchronization: Indicates when a distributed a variable will be\n",
      " |          aggregated. Accepted values are constants defined in the class\n",
      " |          `tf.VariableSynchronization`. By default the synchronization is set to\n",
      " |          `AUTO` and the current `DistributionStrategy` chooses\n",
      " |          when to synchronize. If `synchronization` is set to `ON_READ`,\n",
      " |          `trainable` must not be set to `True`.\n",
      " |        aggregation: Indicates how a distributed variable will be aggregated.\n",
      " |          Accepted values are constants defined in the class\n",
      " |          `tf.VariableAggregation`.\n",
      " |        **kwargs: Additional keyword arguments. Accepted values are `getter`,\n",
      " |          `collections`, `experimental_autocast` and `caching_device`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The created variable. Usually either a `Variable` or `ResourceVariable`\n",
      " |        instance. If `partitioner` is not `None`, a `PartitionedVariable`\n",
      " |        instance is returned.\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called with partitioned variable regularization and\n",
      " |          eager execution is enabled.\n",
      " |        ValueError: When giving unsupported dtype and no initializer or when\n",
      " |          trainable has been set to True with synchronization set as `ON_READ`.\n",
      " |  \n",
      " |  apply(self, inputs, *args, **kwargs)\n",
      " |      Deprecated, do NOT use! (deprecated)\n",
      " |      \n",
      " |      Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.\n",
      " |      Instructions for updating:\n",
      " |      Please use `layer.__call__` method instead.\n",
      " |      \n",
      " |      This is an alias of `self.__call__`.\n",
      " |      \n",
      " |      Arguments:\n",
      " |        inputs: Input tensor(s).\n",
      " |        *args: additional positional arguments to be passed to `self.call`.\n",
      " |        **kwargs: additional keyword arguments to be passed to `self.call`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Output tensor(s).\n",
      " |  \n",
      " |  compute_mask(self, inputs, mask=None)\n",
      " |      Computes an output mask tensor.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          inputs: Tensor or list of tensors.\n",
      " |          mask: Tensor or list of tensors.\n",
      " |      \n",
      " |      Returns:\n",
      " |          None or a tensor (or list of tensors,\n",
      " |              one per output tensor of the layer).\n",
      " |  \n",
      " |  compute_output_signature(self, input_signature)\n",
      " |      Compute the output tensor signature of the layer based on the inputs.\n",
      " |      \n",
      " |      Unlike a TensorShape object, a TensorSpec object contains both shape\n",
      " |      and dtype information for a tensor. This method allows layers to provide\n",
      " |      output dtype information if it is different from the input dtype.\n",
      " |      For any layer that doesn't implement this function,\n",
      " |      the framework will fall back to use `compute_output_shape`, and will\n",
      " |      assume that the output dtype matches the input dtype.\n",
      " |      \n",
      " |      Args:\n",
      " |        input_signature: Single TensorSpec or nested structure of TensorSpec\n",
      " |          objects, describing a candidate input for the layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Single TensorSpec or nested structure of TensorSpec objects, describing\n",
      " |          how the layer would transform the provided input.\n",
      " |      \n",
      " |      Raises:\n",
      " |        TypeError: If input_signature contains a non-TensorSpec object.\n",
      " |  \n",
      " |  count_params(self)\n",
      " |      Count the total number of scalars composing the weights.\n",
      " |      \n",
      " |      Returns:\n",
      " |          An integer count.\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValueError: if the layer isn't yet built\n",
      " |            (in which case its weights aren't yet defined).\n",
      " |  \n",
      " |  get_input_at(self, node_index)\n",
      " |      Retrieves the input tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A tensor (or list of tensors if the layer has multiple inputs).\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |  \n",
      " |  get_input_mask_at(self, node_index)\n",
      " |      Retrieves the input mask tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A mask tensor\n",
      " |          (or list of tensors if the layer has multiple inputs).\n",
      " |  \n",
      " |  get_input_shape_at(self, node_index)\n",
      " |      Retrieves the input shape(s) of a layer at a given node.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A shape tuple\n",
      " |          (or list of shape tuples if the layer has multiple inputs).\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |  \n",
      " |  get_losses_for(self, inputs)\n",
      " |      Retrieves losses relevant to a specific set of inputs.\n",
      " |      \n",
      " |      Arguments:\n",
      " |        inputs: Input tensor or list/tuple of input tensors.\n",
      " |      \n",
      " |      Returns:\n",
      " |        List of loss tensors of the layer that depend on `inputs`.\n",
      " |  \n",
      " |  get_output_at(self, node_index)\n",
      " |      Retrieves the output tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A tensor (or list of tensors if the layer has multiple outputs).\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |  \n",
      " |  get_output_mask_at(self, node_index)\n",
      " |      Retrieves the output mask tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A mask tensor\n",
      " |          (or list of tensors if the layer has multiple outputs).\n",
      " |  \n",
      " |  get_output_shape_at(self, node_index)\n",
      " |      Retrieves the output shape(s) of a layer at a given node.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A shape tuple\n",
      " |          (or list of shape tuples if the layer has multiple outputs).\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |  \n",
      " |  get_updates_for(self, inputs)\n",
      " |      Retrieves updates relevant to a specific set of inputs.\n",
      " |      \n",
      " |      Arguments:\n",
      " |        inputs: Input tensor or list/tuple of input tensors.\n",
      " |      \n",
      " |      Returns:\n",
      " |        List of update ops of the layer that depend on `inputs`.\n",
      " |  \n",
      " |  get_weights(self)\n",
      " |      Returns the current weights of the layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Weights values as a list of numpy arrays.\n",
      " |  \n",
      " |  set_weights(self, weights)\n",
      " |      Sets the weights of the layer, from Numpy arrays.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          weights: a list of Numpy arrays. The number\n",
      " |              of arrays and their shape must match\n",
      " |              number of the dimensions of the weights\n",
      " |              of the layer (i.e. it should match the\n",
      " |              output of `get_weights`).\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValueError: If the provided weights list does not match the\n",
      " |              layer's specifications.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from tensorflow.python.keras.engine.base_layer.Layer:\n",
      " |  \n",
      " |  from_config(config) from builtins.type\n",
      " |      Creates a layer from its config.\n",
      " |      \n",
      " |      This method is the reverse of `get_config`,\n",
      " |      capable of instantiating the same layer from the config\n",
      " |      dictionary. It does not handle layer connectivity\n",
      " |      (handled by Network), nor weights (handled by `set_weights`).\n",
      " |      \n",
      " |      Arguments:\n",
      " |          config: A Python dictionary, typically the\n",
      " |              output of get_config.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A layer instance.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from tensorflow.python.keras.engine.base_layer.Layer:\n",
      " |  \n",
      " |  activity_regularizer\n",
      " |      Optional regularizer function for the output of this layer.\n",
      " |  \n",
      " |  dtype\n",
      " |  \n",
      " |  dynamic\n",
      " |  \n",
      " |  inbound_nodes\n",
      " |      Deprecated, do NOT use! Only for compatibility with external Keras.\n",
      " |  \n",
      " |  input\n",
      " |      Retrieves the input tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one input,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Input tensor or list of input tensors.\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |        AttributeError: If no inbound nodes are found.\n",
      " |  \n",
      " |  input_mask\n",
      " |      Retrieves the input mask tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one inbound node,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Input mask tensor (potentially None) or list of input\n",
      " |          mask tensors.\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: if the layer is connected to\n",
      " |          more than one incoming layers.\n",
      " |  \n",
      " |  input_shape\n",
      " |      Retrieves the input shape(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one input,\n",
      " |      i.e. if it is connected to one incoming layer, or if all inputs\n",
      " |      have the same shape.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Input shape, as an integer shape tuple\n",
      " |          (or list of shape tuples, one tuple per input tensor).\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: if the layer has no defined input_shape.\n",
      " |          RuntimeError: if called in Eager mode.\n",
      " |  \n",
      " |  input_spec\n",
      " |  \n",
      " |  losses\n",
      " |      Losses which are associated with this `Layer`.\n",
      " |      \n",
      " |      Variable regularization tensors are created when this property is accessed,\n",
      " |      so it is eager safe: accessing `losses` under a `tf.GradientTape` will\n",
      " |      propagate gradients back to the corresponding variables.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of tensors.\n",
      " |  \n",
      " |  metrics\n",
      " |  \n",
      " |  name\n",
      " |      Returns the name of this module as passed or determined in the ctor.\n",
      " |      \n",
      " |      NOTE: This is not the same as the `self.name_scope.name` which includes\n",
      " |      parent module names.\n",
      " |  \n",
      " |  non_trainable_variables\n",
      " |  \n",
      " |  non_trainable_weights\n",
      " |  \n",
      " |  outbound_nodes\n",
      " |      Deprecated, do NOT use! Only for compatibility with external Keras.\n",
      " |  \n",
      " |  output\n",
      " |      Retrieves the output tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one output,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Output tensor or list of output tensors.\n",
      " |      \n",
      " |      Raises:\n",
      " |        AttributeError: if the layer is connected to more than one incoming\n",
      " |          layers.\n",
      " |        RuntimeError: if called in Eager mode.\n",
      " |  \n",
      " |  output_mask\n",
      " |      Retrieves the output mask tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one inbound node,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Output mask tensor (potentially None) or list of output\n",
      " |          mask tensors.\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: if the layer is connected to\n",
      " |          more than one incoming layers.\n",
      " |  \n",
      " |  output_shape\n",
      " |      Retrieves the output shape(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has one output,\n",
      " |      or if all outputs have the same shape.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Output shape, as an integer shape tuple\n",
      " |          (or list of shape tuples, one tuple per output tensor).\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: if the layer has no defined output shape.\n",
      " |          RuntimeError: if called in Eager mode.\n",
      " |  \n",
      " |  stateful\n",
      " |  \n",
      " |  trainable\n",
      " |  \n",
      " |  trainable_variables\n",
      " |      Sequence of trainable variables owned by this module and its submodules.\n",
      " |      \n",
      " |      Note: this method uses reflection to find variables on the current instance\n",
      " |      and submodules. For performance reasons you may wish to cache the result\n",
      " |      of calling this method if you don't expect the return value to change.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A sequence of variables for the current module (sorted by attribute\n",
      " |        name) followed by variables from all submodules recursively (breadth\n",
      " |        first).\n",
      " |  \n",
      " |  trainable_weights\n",
      " |  \n",
      " |  updates\n",
      " |  \n",
      " |  variables\n",
      " |      Returns the list of all layer variables/weights.\n",
      " |      \n",
      " |      Alias of `self.weights`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of variables.\n",
      " |  \n",
      " |  weights\n",
      " |      Returns the list of all layer variables/weights.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of variables.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from tensorflow.python.module.module.Module:\n",
      " |  \n",
      " |  with_name_scope(method) from builtins.type\n",
      " |      Decorator to automatically enter the module name scope.\n",
      " |      \n",
      " |      ```\n",
      " |      class MyModule(tf.Module):\n",
      " |        @tf.Module.with_name_scope\n",
      " |        def __call__(self, x):\n",
      " |          if not hasattr(self, 'w'):\n",
      " |            self.w = tf.Variable(tf.random.normal([x.shape[1], 64]))\n",
      " |          return tf.matmul(x, self.w)\n",
      " |      ```\n",
      " |      \n",
      " |      Using the above module would produce `tf.Variable`s and `tf.Tensor`s whose\n",
      " |      names included the module name:\n",
      " |      \n",
      " |      ```\n",
      " |      mod = MyModule()\n",
      " |      mod(tf.ones([8, 32]))\n",
      " |      # ==> <tf.Tensor: ...>\n",
      " |      mod.w\n",
      " |      # ==> <tf.Variable ...'my_module/w:0'>\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        method: The method to wrap.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The original method wrapped such that it enters the module's name scope.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from tensorflow.python.module.module.Module:\n",
      " |  \n",
      " |  name_scope\n",
      " |      Returns a `tf.name_scope` instance for this class.\n",
      " |  \n",
      " |  submodules\n",
      " |      Sequence of all sub-modules.\n",
      " |      \n",
      " |      Submodules are modules which are properties of this module, or found as\n",
      " |      properties of modules which are properties of this module (and so on).\n",
      " |      \n",
      " |      ```\n",
      " |      a = tf.Module()\n",
      " |      b = tf.Module()\n",
      " |      c = tf.Module()\n",
      " |      a.b = b\n",
      " |      b.c = c\n",
      " |      assert list(a.submodules) == [b, c]\n",
      " |      assert list(b.submodules) == [c]\n",
      " |      assert list(c.submodules) == []\n",
      " |      ```\n",
      " |      \n",
      " |      Returns:\n",
      " |        A sequence of all submodules.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from tensorflow.python.training.tracking.base.Trackable:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20640, 8)\n",
      "(20640,)\n"
     ]
    }
   ],
   "source": [
    "housing = fetch_california_housing()\n",
    "print(housing.data.shape)\n",
    "print(housing.target.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据集划分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11610, 8) (11610,)\n",
      "(3870, 8) (3870,)\n",
      "(5160, 8) (5160,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "#默认训练集：测试集 =3 ：1 \n",
    "x_train_all,x_test,y_train_all,y_test = train_test_split(housing.data,housing.target,random_state = 7,test_size = 0.25)\n",
    "x_train,x_valid,y_train,y_valid = train_test_split(x_train_all,y_train_all,random_state = 11,test_size = 0.25)\n",
    "print(x_train.shape,y_train.shape)\n",
    "print(x_valid.shape,y_valid.shape)\n",
    "print(x_test.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据归一化处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train)\n",
    "x_valid_scaled = scaler.transform(x_valid)\n",
    "x_test_scaled  = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([4.5417706e-05 6.7153489e-03 6.9314718e-01 5.0067153e+00 1.0000046e+01], shape=(5,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#使用 labmda 自定义 简单无参数层次（无w ，b）\n",
    "customized_softplus = tf.keras.layers.Lambda(lambda x: tf.nn.softplus(x))\n",
    "print(customized_softplus([-10.,-5.,0.,5.,10.]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "customized_dense_layer_2 (Cu (None, 30)                270       \n",
      "_________________________________________________________________\n",
      "customized_dense_layer_3 (Cu (None, 1)                 31        \n",
      "=================================================================\n",
      "Total params: 301\n",
      "Trainable params: 301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#自定义损失函数loss -- mse\n",
    "def customized_mse(y_true,y_pred):\n",
    "    return tf.reduce_mean(tf.square(y_pred - y_true))\n",
    "\n",
    "#自定义全连接层Dense layer\n",
    "class CustomizedDenseLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self,units,activation=None,**kwargs):\n",
    "        self.units = units\n",
    "        self.activation = tf.keras.layers.Activation(activation)\n",
    "        super(CustomizedDenseLayer,self).__init__(**kwargs)\n",
    "        \n",
    "    def build(self,input_shape):\n",
    "        '''构建所需的参数'''\n",
    "        #权值矩阵\n",
    "        self.kernel = self.add_weight(name = 'kernal',\n",
    "                                     shape = (input_shape[1],self.units),\n",
    "                                      initializer = 'uniform',\n",
    "                                      trainable = True)\n",
    "        self.bias = self.add_weight(name = 'bias',\n",
    "                                    shape = (self.units,),\n",
    "                                    initializer = 'zeros',\n",
    "                                    trainable = True)\n",
    "        super(CustomizedDenseLayer,self).build(input_shape)\n",
    "        \n",
    "    def call(self,x):\n",
    "        #x * w + b\n",
    "        return self.activation(x @ self.kernel + self.bias)\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    CustomizedDenseLayer(30,activation = \"relu\",input_shape = x_train_scaled.shape[1:]),\n",
    "    CustomizedDenseLayer(1),\n",
    "    customized_softplus,\n",
    "    #等价于 tf.keras.layers.Dense(1,activation = \"softplus\") 或 tf.keras.layers.Dense(1),tf.keras.layers.Activation(\"softplus\")\n",
    "])\n",
    "model.summary()\n",
    "model.compile(loss=customized_mse,optimizer=\"Adam\",metrics=[\"mean_squared_error\"]) #sgd随机梯度下降，mean_squared_error均方误差\n",
    "callbacks = [tf.keras.callbacks.EarlyStopping(patience= 5,min_delta = 1e-3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11610 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "11610/11610 [==============================] - 2s 130us/sample - loss: 2.0079 - mean_squared_error: 2.0079 - val_loss: 0.6789 - val_mean_squared_error: 0.6789\n",
      "Epoch 2/100\n",
      "11610/11610 [==============================] - 1s 78us/sample - loss: 0.5053 - mean_squared_error: 0.5053 - val_loss: 0.4716 - val_mean_squared_error: 0.4716\n",
      "Epoch 3/100\n",
      "11610/11610 [==============================] - 1s 79us/sample - loss: 0.4319 - mean_squared_error: 0.4319 - val_loss: 0.4420 - val_mean_squared_error: 0.4420\n",
      "Epoch 4/100\n",
      "11610/11610 [==============================] - 1s 79us/sample - loss: 0.4137 - mean_squared_error: 0.4137 - val_loss: 0.4235 - val_mean_squared_error: 0.4235\n",
      "Epoch 5/100\n",
      "11610/11610 [==============================] - 1s 80us/sample - loss: 0.4041 - mean_squared_error: 0.4041 - val_loss: 0.4162 - val_mean_squared_error: 0.4162\n",
      "Epoch 6/100\n",
      "11610/11610 [==============================] - 1s 83us/sample - loss: 0.3968 - mean_squared_error: 0.3968 - val_loss: 0.4052 - val_mean_squared_error: 0.4052\n",
      "Epoch 7/100\n",
      "11610/11610 [==============================] - 1s 80us/sample - loss: 0.3908 - mean_squared_error: 0.3908 - val_loss: 0.3988 - val_mean_squared_error: 0.3988\n",
      "Epoch 8/100\n",
      "11610/11610 [==============================] - 1s 80us/sample - loss: 0.3831 - mean_squared_error: 0.3831 - val_loss: 0.3891 - val_mean_squared_error: 0.3891\n",
      "Epoch 9/100\n",
      "11610/11610 [==============================] - 1s 79us/sample - loss: 0.3770 - mean_squared_error: 0.3770 - val_loss: 0.3922 - val_mean_squared_error: 0.3922\n",
      "Epoch 10/100\n",
      "11610/11610 [==============================] - 1s 83us/sample - loss: 0.3733 - mean_squared_error: 0.3733 - val_loss: 0.3786 - val_mean_squared_error: 0.3786\n",
      "Epoch 11/100\n",
      "11610/11610 [==============================] - 1s 83us/sample - loss: 0.3666 - mean_squared_error: 0.3666 - val_loss: 0.3737 - val_mean_squared_error: 0.3737\n",
      "Epoch 12/100\n",
      "11610/11610 [==============================] - 1s 80us/sample - loss: 0.3655 - mean_squared_error: 0.3655 - val_loss: 0.3703 - val_mean_squared_error: 0.3703\n",
      "Epoch 13/100\n",
      "11610/11610 [==============================] - 1s 81us/sample - loss: 0.3614 - mean_squared_error: 0.3614 - val_loss: 0.3649 - val_mean_squared_error: 0.3649\n",
      "Epoch 14/100\n",
      "11610/11610 [==============================] - 1s 89us/sample - loss: 0.3562 - mean_squared_error: 0.3562 - val_loss: 0.3637 - val_mean_squared_error: 0.3637\n",
      "Epoch 15/100\n",
      "11610/11610 [==============================] - 1s 81us/sample - loss: 0.3530 - mean_squared_error: 0.3530 - val_loss: 0.3572 - val_mean_squared_error: 0.3572\n",
      "Epoch 16/100\n",
      "11610/11610 [==============================] - 1s 85us/sample - loss: 0.3524 - mean_squared_error: 0.3524 - val_loss: 0.3587 - val_mean_squared_error: 0.3587\n",
      "Epoch 17/100\n",
      "11610/11610 [==============================] - 1s 110us/sample - loss: 0.3482 - mean_squared_error: 0.3482 - val_loss: 0.3526 - val_mean_squared_error: 0.3526\n",
      "Epoch 18/100\n",
      "11610/11610 [==============================] - 1s 81us/sample - loss: 0.3455 - mean_squared_error: 0.3455 - val_loss: 0.3512 - val_mean_squared_error: 0.3512\n",
      "Epoch 19/100\n",
      "11610/11610 [==============================] - 1s 84us/sample - loss: 0.3453 - mean_squared_error: 0.3453 - val_loss: 0.3494 - val_mean_squared_error: 0.3494\n",
      "Epoch 20/100\n",
      "11610/11610 [==============================] - 1s 80us/sample - loss: 0.3412 - mean_squared_error: 0.3412 - val_loss: 0.3454 - val_mean_squared_error: 0.3454\n",
      "Epoch 21/100\n",
      "11610/11610 [==============================] - 1s 80us/sample - loss: 0.3422 - mean_squared_error: 0.3422 - val_loss: 0.3458 - val_mean_squared_error: 0.3458\n",
      "Epoch 22/100\n",
      "11610/11610 [==============================] - 1s 89us/sample - loss: 0.3361 - mean_squared_error: 0.3361 - val_loss: 0.3470 - val_mean_squared_error: 0.3470\n",
      "Epoch 23/100\n",
      "11610/11610 [==============================] - 1s 81us/sample - loss: 0.3354 - mean_squared_error: 0.3354 - val_loss: 0.3429 - val_mean_squared_error: 0.3429\n",
      "Epoch 24/100\n",
      "11610/11610 [==============================] - 1s 80us/sample - loss: 0.3337 - mean_squared_error: 0.3337 - val_loss: 0.3398 - val_mean_squared_error: 0.3398\n",
      "Epoch 25/100\n",
      "11610/11610 [==============================] - 1s 81us/sample - loss: 0.3327 - mean_squared_error: 0.3327 - val_loss: 0.3441 - val_mean_squared_error: 0.3441\n",
      "Epoch 26/100\n",
      "11610/11610 [==============================] - 1s 81us/sample - loss: 0.3315 - mean_squared_error: 0.3315 - val_loss: 0.3377 - val_mean_squared_error: 0.3377\n",
      "Epoch 27/100\n",
      "11610/11610 [==============================] - 1s 83us/sample - loss: 0.3315 - mean_squared_error: 0.3315 - val_loss: 0.3412 - val_mean_squared_error: 0.3412\n",
      "Epoch 28/100\n",
      "11610/11610 [==============================] - 1s 81us/sample - loss: 0.3270 - mean_squared_error: 0.3270 - val_loss: 0.3381 - val_mean_squared_error: 0.3381\n",
      "Epoch 29/100\n",
      "11610/11610 [==============================] - 1s 88us/sample - loss: 0.3263 - mean_squared_error: 0.3263 - val_loss: 0.3323 - val_mean_squared_error: 0.3323\n",
      "Epoch 30/100\n",
      "11610/11610 [==============================] - 1s 84us/sample - loss: 0.3243 - mean_squared_error: 0.3243 - val_loss: 0.3354 - val_mean_squared_error: 0.3354\n",
      "Epoch 31/100\n",
      "11610/11610 [==============================] - 1s 84us/sample - loss: 0.3246 - mean_squared_error: 0.3246 - val_loss: 0.3340 - val_mean_squared_error: 0.3340\n",
      "Epoch 32/100\n",
      "11610/11610 [==============================] - 1s 100us/sample - loss: 0.3296 - mean_squared_error: 0.3296 - val_loss: 0.3328 - val_mean_squared_error: 0.3328\n",
      "Epoch 33/100\n",
      "11610/11610 [==============================] - 1s 87us/sample - loss: 0.3215 - mean_squared_error: 0.3215 - val_loss: 0.3336 - val_mean_squared_error: 0.3336\n",
      "Epoch 34/100\n",
      "11610/11610 [==============================] - 1s 83us/sample - loss: 0.3222 - mean_squared_error: 0.3222 - val_loss: 0.3268 - val_mean_squared_error: 0.3268\n",
      "Epoch 35/100\n",
      "11610/11610 [==============================] - 1s 85us/sample - loss: 0.3222 - mean_squared_error: 0.3222 - val_loss: 0.3267 - val_mean_squared_error: 0.3267\n",
      "Epoch 36/100\n",
      "11610/11610 [==============================] - 1s 84us/sample - loss: 0.3212 - mean_squared_error: 0.3212 - val_loss: 0.3270 - val_mean_squared_error: 0.3270\n",
      "Epoch 37/100\n",
      "11610/11610 [==============================] - 1s 103us/sample - loss: 0.3222 - mean_squared_error: 0.3222 - val_loss: 0.3288 - val_mean_squared_error: 0.3288\n",
      "Epoch 38/100\n",
      "11610/11610 [==============================] - 1s 89us/sample - loss: 0.3191 - mean_squared_error: 0.3191 - val_loss: 0.3265 - val_mean_squared_error: 0.3265\n",
      "Epoch 39/100\n",
      "11610/11610 [==============================] - 1s 87us/sample - loss: 0.3209 - mean_squared_error: 0.3209 - val_loss: 0.3261 - val_mean_squared_error: 0.3261\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train_scaled,y_train,validation_data=(x_valid_scaled,y_valid),epochs=100,callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_learning_curves(history):\n",
    "    pd.DataFrame(history.history).plot(figsize=(8,5))\n",
    "    plt.grid(True)\n",
    "    plt.gca().set_ylim(0,1)\n",
    "    plt.show()\n",
    "plot_learning_curves(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5160/5160 [==============================] - 0s 42us/sample - loss: 0.3321 - mean_squared_error: 0.3321\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.33214976196141205, 0.3321497]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test_scaled,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
